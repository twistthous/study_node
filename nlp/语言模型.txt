nlp语言模型
	计算一个词语序列构成一个句子的概率，或者说计算一个词语序列的联合概率
	可以用来判断一句话出现的概率高不高，符不符合我们的表达习惯，是否通顺，一句话是否正确
	
	概率语言模型
		Unigram models 一元文法统计模型
			p(s)=p(w1)*p(w2)*...*p(wn)
			这个式子成立的条件是有一个假设，就是条件无关假设，我们认为每个词都是条件无关的
				=> 今天的天气非常晴朗 p(今天的天气非常晴朗。)=p(今)*p(天)*p(的)*...*p(。)
		
			二元文法统计模型
				我喝水 我吃水
				p(我喝水)=p(我)*p(喝)*p(水)
				p(我吃水)=p(我)*p(吃)*p(水) 用一元统计模型返回的概率差不多，甚至可能会相同，会得到我们不想要的内容
			
				p(s)=p(w1|</s>)*p(w2|w1)*p(w3|w2)*...*p(</s>|wn)
				p(s)=p(<s>我喝水</s>)=p(我|<s>)*p(喝|我)*p(水|喝)*p(<s>|水)
				
				二元语言模型也能比一元语言模型能够更好的get到两个词语之间的关系信息
				有点可以进行前后关联
			
		N-gram 语言模型 （N元模型）
			对于3-gram p(wn|w1w2...wn-1)=p(wn|wn-1...w1)
			n大于3基本就无法处理了，参数空间太大，另外它不能表示词与词之间的关联性
			
		词向量与word2vec
			词向量(word mbedding/词嵌入) 来自词汇表的单词或短语被映射到实数的向量
			
			word2vec 是为一群用来产生词向量的相关模型。这些模型为浅而双层的神经网络，用来训练以重新构建语言学之词文本
			
			cbow模型 
				输入层 映射层 输出层
				是二叉树结构
				二叉树结构应用到word2vex中被称之为Hierarchical Softmax
				输入是词向量
				
			skip-gram
				与cbow模型正好相反
				输入层 输出层 映射层
				二叉树结构
				输出是词向量
				生成了哈夫曼树
			
		文本处理方法
			1.数据清洗 (去掉无意义的标签、url、符号等)
			2.分词、大小写转换、添加句首句尾、词性标注
			3.统计词频、抽取文本特征、特征选择、计算特征权重、归一化
			4.划分训练集、测试集
	
		do what
			1.预测字符串概率
			2.动机
			3.如何计算
			
	